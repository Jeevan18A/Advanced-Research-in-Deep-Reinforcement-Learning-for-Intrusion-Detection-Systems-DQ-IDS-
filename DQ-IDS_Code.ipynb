{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-vyPQBegPJN",
        "outputId": "35db9e8d-3b10-4fc9-ad9c-96eb64e33969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "8IYNDbNNiLvf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "685QEUjviXYR",
        "outputId": "7aeb4618-30ff-4c23-a6b9-35667f0a7536"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-26297bbc-bbc9-4628-8e15-99d490dfffa5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-26297bbc-bbc9-4628-8e15-99d490dfffa5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ajeevanreddy\",\"key\":\"f96c72b6ecc2548efe34438739e65dcf\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "UH1xMObIib_Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d solarmainframe/ids-intrusion-csv -p /content/drive/MyDrive/RL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHLhcfR4iiWH",
        "outputId": "ccdf7202-14ad-4036-f6e6-57f8ce4bdaef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/solarmainframe/ids-intrusion-csv\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading ids-intrusion-csv.zip to /content/drive/MyDrive/RL\n",
            " 99% 1.58G/1.60G [00:10<00:00, 191MB/s]\n",
            "100% 1.60G/1.60G [00:10<00:00, 170MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/RL/ids-intrusion-csv.zip -d /content/drive/MyDrive/RL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jghxx8xEit8n",
        "outputId": "ac1aa191-be09-4f9f-c7e6-5157b98eb274"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/RL/ids-intrusion-csv.zip\n",
            "  inflating: /content/drive/MyDrive/RL/02-14-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-15-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-16-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-20-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-21-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-22-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-23-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/02-28-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/03-01-2018.csv  \n",
            "  inflating: /content/drive/MyDrive/RL/03-02-2018.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Import Libraries\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "OfALwfePi1D_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Load Dataset\n",
        "data_path = \"/content/drive/MyDrive/RL/\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "\n",
        "print(\"Total files found:\", len(all_files))\n",
        "\n",
        "# Just read the first file for inspection\n",
        "sample_file = all_files[0]\n",
        "df = pd.read_csv(sample_file)\n",
        "\n",
        "print(\"File:\", sample_file)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns[:20])  # preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKGZ8Isij3V9",
        "outputId": "bb888cbc-08d5-404a-9a89-e871240c1690"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files found: 10\n",
            "File: /content/drive/MyDrive/RL/02-14-2018.csv\n",
            "Shape: (1048575, 80)\n",
            "Columns: Index(['Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts',\n",
            "       'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n",
            "       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
            "       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n",
            "       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n",
            "       'Flow IAT Std'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/RL/\"\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "\n",
        "print(\"Total files found:\", len(all_files))\n",
        "\n",
        "for i, file in enumerate(all_files[:5]):  # limit to 5 files for preview\n",
        "    print(f\"\\nReading file {i+1}: {file}\")\n",
        "    df = pd.read_csv(file, nrows=5)  # only read first 5 rows\n",
        "    print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86vNU-IimbLN",
        "outputId": "4b91648e-68f1-4dd5-a425-2cba2a825087"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files found: 10\n",
            "\n",
            "Reading file 1: /content/drive/MyDrive/RL/02-14-2018.csv\n",
            "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
            "0         0         0  14/02/2018 08:31:01      112641719             3   \n",
            "1         0         0  14/02/2018 08:33:50      112641466             3   \n",
            "2         0         0  14/02/2018 08:36:39      112638623             3   \n",
            "3        22         6  14/02/2018 08:40:13        6453966            15   \n",
            "4        22         6  14/02/2018 08:40:23        8804066            14   \n",
            "\n",
            "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
            "0             0                0                0                0   \n",
            "1             0                0                0                0   \n",
            "2             0                0                0                0   \n",
            "3            10             1239             2273              744   \n",
            "4            11             1143             2209              744   \n",
            "\n",
            "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
            "0                0  ...                 0            0           0   \n",
            "1                0  ...                 0            0           0   \n",
            "2                0  ...                 0            0           0   \n",
            "3                0  ...                32            0           0   \n",
            "4                0  ...                32            0           0   \n",
            "\n",
            "   Active Max  Active Min   Idle Mean    Idle Std  Idle Max  Idle Min   Label  \n",
            "0           0           0  56320859.5  139.300036  56320958  56320761  Benign  \n",
            "1           0           0  56320733.0  114.551299  56320814  56320652  Benign  \n",
            "2           0           0  56319311.5  301.934596  56319525  56319098  Benign  \n",
            "3           0           0         0.0    0.000000         0         0  Benign  \n",
            "4           0           0         0.0    0.000000         0         0  Benign  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "\n",
            "Reading file 2: /content/drive/MyDrive/RL/02-15-2018.csv\n",
            "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
            "0         0         0  15/02/2018 08:25:18      112641158             3   \n",
            "1        22         6  15/02/2018 08:29:05       37366762            14   \n",
            "2     47514         6  15/02/2018 08:29:42            543             2   \n",
            "3         0         0  15/02/2018 08:28:07      112640703             3   \n",
            "4         0         0  15/02/2018 08:30:56      112640874             3   \n",
            "\n",
            "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
            "0             0                0                0                0   \n",
            "1            12             2168             2993              712   \n",
            "2             0               64                0               64   \n",
            "3             0                0                0                0   \n",
            "4             0                0                0                0   \n",
            "\n",
            "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean     Active Std  \\\n",
            "0                0  ...                 0            0       0.000000   \n",
            "1                0  ...                32      1024353  649038.754495   \n",
            "2                0  ...                32            0       0.000000   \n",
            "3                0  ...                 0            0       0.000000   \n",
            "4                0  ...                 0            0       0.000000   \n",
            "\n",
            "   Active Max  Active Min   Idle Mean      Idle Std  Idle Max  Idle Min  \\\n",
            "0           0           0  56320579.0  7.042784e+02  56321077  56320081   \n",
            "1     1601183      321569  11431221.0  3.644991e+06  15617415   8960247   \n",
            "2           0           0         0.0  0.000000e+00         0         0   \n",
            "3           0           0  56320351.5  3.669884e+02  56320611  56320092   \n",
            "4           0           0  56320437.0  7.198347e+02  56320946  56319928   \n",
            "\n",
            "    Label  \n",
            "0  Benign  \n",
            "1  Benign  \n",
            "2  Benign  \n",
            "3  Benign  \n",
            "4  Benign  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "\n",
            "Reading file 3: /content/drive/MyDrive/RL/02-16-2018.csv\n",
            "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
            "0         0         0  16/02/2018 08:27:23      112640768             3   \n",
            "1         0         0  16/02/2018 08:30:12      112641773             3   \n",
            "2     35605         6  16/02/2018 08:26:55       20784143            23   \n",
            "3         0         0  16/02/2018 08:33:01      112640836             3   \n",
            "4        23         6  16/02/2018 08:27:59             20             1   \n",
            "\n",
            "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
            "0             0                0                0                0   \n",
            "1             0                0                0                0   \n",
            "2            44             2416             1344              240   \n",
            "3             0                0                0                0   \n",
            "4             1                0                0                0   \n",
            "\n",
            "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
            "0                0  ...                 0            0           0   \n",
            "1                0  ...                 0            0           0   \n",
            "2               64  ...                20      2624734           0   \n",
            "3                0  ...                 0            0           0   \n",
            "4                0  ...                20            0           0   \n",
            "\n",
            "   Active Max  Active Min  Idle Mean    Idle Std  Idle Max  Idle Min   Label  \n",
            "0           0           0   56300000  138.592929  56300000  56300000  Benign  \n",
            "1           0           0   56300000  263.750829  56300000  56300000  Benign  \n",
            "2     2624734     2624734    9058214    0.000000   9058214   9058214  Benign  \n",
            "3           0           0   56300000   82.024387  56300000  56300000  Benign  \n",
            "4           0           0          0    0.000000         0         0  Benign  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "\n",
            "Reading file 4: /content/drive/MyDrive/RL/02-20-2018.csv\n",
            "                                  Flow ID          Src IP  Src Port  \\\n",
            "0  172.31.69.25-94.231.103.172-22-45498-6  94.231.103.172     45498   \n",
            "1                   8.0.6.4-8.6.0.1-0-0-0         8.6.0.1         0   \n",
            "2                   8.0.6.4-8.6.0.1-0-0-0         8.6.0.1         0   \n",
            "3                   8.0.6.4-8.6.0.1-0-0-0         8.6.0.1         0   \n",
            "4                   8.0.6.4-8.6.0.1-0-0-0         8.6.0.1         0   \n",
            "\n",
            "         Dst IP  Dst Port  Protocol            Timestamp  Flow Duration  \\\n",
            "0  172.31.69.25        22         6  20/02/2018 08:34:07         888751   \n",
            "1       8.0.6.4         0         0  20/02/2018 08:33:22      112642816   \n",
            "2       8.0.6.4         0         0  20/02/2018 08:36:11      112642712   \n",
            "3       8.0.6.4         0         0  20/02/2018 08:39:00      112642648   \n",
            "4       8.0.6.4         0         0  20/02/2018 08:41:49      112642702   \n",
            "\n",
            "   Tot Fwd Pkts  Tot Bwd Pkts  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
            "0            11            11  ...                32            0           0   \n",
            "1             3             0  ...                 0            0           0   \n",
            "2             3             0  ...                 0            0           0   \n",
            "3             3             0  ...                 0            0           0   \n",
            "4             3             0  ...                 0            0           0   \n",
            "\n",
            "   Active Max  Active Min   Idle Mean   Idle Std    Idle Max    Idle Min  \\\n",
            "0           0           0         0.0   0.000000         0.0         0.0   \n",
            "1           0           0  56300000.0   7.071068  56300000.0  56300000.0   \n",
            "2           0           0  56300000.0  18.384776  56300000.0  56300000.0   \n",
            "3           0           0  56300000.0   5.656854  56300000.0  56300000.0   \n",
            "4           0           0  56300000.0  65.053824  56300000.0  56300000.0   \n",
            "\n",
            "    Label  \n",
            "0  Benign  \n",
            "1  Benign  \n",
            "2  Benign  \n",
            "3  Benign  \n",
            "4  Benign  \n",
            "\n",
            "[5 rows x 84 columns]\n",
            "\n",
            "Reading file 5: /content/drive/MyDrive/RL/02-21-2018.csv\n",
            "   Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
            "0        80         6  21/02/2018 08:33:25          37953             5   \n",
            "1       500        17  21/02/2018 08:33:06      117573474             3   \n",
            "2       500        17  21/02/2018 08:33:06      117573474             3   \n",
            "3       500        17  21/02/2018 08:33:11       99743998             5   \n",
            "4       500        17  21/02/2018 08:33:11       99743999             5   \n",
            "\n",
            "   Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
            "0             3              135              127              135   \n",
            "1             0             1500                0              500   \n",
            "2             0             1500                0              500   \n",
            "3             0             2500                0              500   \n",
            "4             0             2500                0              500   \n",
            "\n",
            "   Fwd Pkt Len Min  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
            "0                0  ...                32            0           0   \n",
            "1              500  ...                 8            0           0   \n",
            "2              500  ...                 8            0           0   \n",
            "3              500  ...                 8      4000290           0   \n",
            "4              500  ...                 8      4000286           0   \n",
            "\n",
            "   Active Max  Active Min  Idle Mean  Idle Std  Idle Max  Idle Min   Label  \n",
            "0           0           0          0         0         0         0  Benign  \n",
            "1           0           0   58800000  23800000  75600000  42000000  Benign  \n",
            "2           0           0   58800000  23800000  75600000  42000000  Benign  \n",
            "3     4000290     4000290   31900000  37900000  75600000   7200397  Benign  \n",
            "4     4000286     4000286   31900000  37900000  75600000   7200399  Benign  \n",
            "\n",
            "[5 rows x 80 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3 (Fixed): Preprocessing\n",
        "\n",
        "# Initialize an empty list to store dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Read and concatenate all CSV files\n",
        "for file in all_files:\n",
        "    try:\n",
        "        # Read each CSV file into a dataframe\n",
        "        df = pd.read_csv(file)\n",
        "        dataframes.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file {file}: {e}\")\n",
        "\n",
        "# Concatenate all dataframes into a single dataframe\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Drop categorical / non-numeric cols not useful for RL agent\n",
        "drop_cols = ['Dst IP', 'Timestamp']  # you can add 'Src IP', etc if present\n",
        "data = data.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "# Convert 'Dst Port' to numeric, coercing errors to NaN\n",
        "data['Dst Port'] = pd.to_numeric(data['Dst Port'], errors='coerce')\n",
        "\n",
        "# Fill NaN values in 'Dst Port' with 0 (or another appropriate value)\n",
        "data['Dst Port'] = data['Dst Port'].fillna(0)\n",
        "\n",
        "# Separate features and labels\n",
        "y = data['Label']\n",
        "X = data.drop(columns=['Label'], errors='ignore')\n",
        "\n",
        "# Identify and drop remaining non-numeric columns\n",
        "non_numeric_cols = X.select_dtypes(exclude=np.number).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    print(f\"Dropping non-numeric columns: {list(non_numeric_cols)}\")\n",
        "    X = X.drop(columns=non_numeric_cols)\n",
        "\n",
        "# Encode labels (Benign = 0, Attacks = 1, or multiclass)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "print(\"Classes:\", le.classes_)  # see all attack types\n",
        "\n",
        "# Normalize numeric features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgcECurwmmBq",
        "outputId": "ededb9e1-46e2-457d-ac86-a2b38b49d58f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1375657977.py:10: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n",
            "/tmp/ipython-input-1375657977.py:10: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n",
            "/tmp/ipython-input-1375657977.py:10: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping non-numeric columns: ['Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Flow ID', 'Src IP']\n",
            "Classes: ['Benign' 'Bot' 'Brute Force -Web' 'Brute Force -XSS' 'DDOS attack-HOIC'\n",
            " 'DDOS attack-LOIC-UDP' 'DDoS attacks-LOIC-HTTP' 'DoS attacks-GoldenEye'\n",
            " 'DoS attacks-Hulk' 'DoS attacks-SlowHTTPTest' 'DoS attacks-Slowloris'\n",
            " 'FTP-BruteForce' 'Infilteration' 'Label' 'SQL Injection' 'SSH-Bruteforce']\n",
            "Train shape: (12986401, 2) Test shape: (3246601, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4 (Fixed): RL Environment\n",
        "\n",
        "class IDSEnv:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.n_samples = len(y)\n",
        "        self.current_index = 0\n",
        "        self.action_space = len(np.unique(y))  # adapt to multiclass\n",
        "        self.state_space = X.shape[1]\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_index = 0\n",
        "        return self.X[self.current_index]\n",
        "\n",
        "    def step(self, action):\n",
        "        true_label = self.y[self.current_index]\n",
        "        reward = 1 if action == true_label else -1\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= self.n_samples\n",
        "        next_state = (\n",
        "            self.X[self.current_index % self.n_samples] if not done else np.zeros_like(self.X[0])\n",
        "        )\n",
        "        return next_state, reward, done, {}"
      ],
      "metadata": {
        "id": "ENo-hJ2km0zX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Advanced DQN Variants\n",
        "\n",
        "# Dueling DQN\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "\n",
        "        # Value and Advantage streams\n",
        "        self.value_stream = nn.Linear(128, 1)\n",
        "        self.advantage_stream = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "        q_vals = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_vals"
      ],
      "metadata": {
        "id": "PgLQdmGso8GW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Agent with Double DQN\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, batch_size=64, buffer_size=50000):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Main & Target Networks\n",
        "        self.policy_net = DuelingDQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = DuelingDQN(state_dim, action_dim).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "\n",
        "        self.update_target()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def act(self, state, epsilon=0.1):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        q_vals = self.policy_net(state)\n",
        "        return torch.argmax(q_vals, dim=1).item()\n",
        "\n",
        "    def remember(self, s, a, r, s_next, done):\n",
        "        self.memory.append((s, a, r, s_next, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Q(s, a)\n",
        "        q_vals = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        # Double DQN target\n",
        "        next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "        next_q_vals = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "        target = rewards + (1 - dones) * self.gamma * next_q_vals\n",
        "\n",
        "        loss = F.mse_loss(q_vals, target.detach())\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "pfARlNlkpBGV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Compact Training Loop\n",
        "\n",
        "env = IDSEnv(X_train, y_train)\n",
        "agent = DQNAgent(env.state_space, env.action_space)\n",
        "\n",
        "episodes = 5   # keep small for testing; increase later if needed\n",
        "steps_per_episode = 2000  # limit steps per episode (instead of all samples)\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(steps_per_episode):\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        # Replay less frequently for speed\n",
        "        if t % 10 == 0:\n",
        "            agent.replay()\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    agent.update_target()\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "    print(f\"Episode {ep+1}/{episodes}, Reward: {total_reward}, Epsilon: {epsilon:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Sd_pwM4A0_",
        "outputId": "f00a1246-92f5-4536-f525-f9b2784dcd96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/5, Reward: -1742, Epsilon: 0.995\n",
            "Episode 2/5, Reward: -1748, Epsilon: 0.990\n",
            "Episode 3/5, Reward: -1742, Epsilon: 0.985\n",
            "Episode 4/5, Reward: -1690, Epsilon: 0.980\n",
            "Episode 5/5, Reward: -1706, Epsilon: 0.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8 (Optimized Testing)\n",
        "\n",
        "def fast_test(agent, X_test, y_test, batch_size=1024):\n",
        "    device = agent.device\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    # Convert test data to tensor\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "\n",
        "    # Predict in batches\n",
        "    agent.policy_net.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_test), batch_size):\n",
        "            batch_X = X_test_tensor[i:i+batch_size]\n",
        "            outputs = agent.policy_net(batch_X)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "            batch_y = y_test[i:i+batch_size]\n",
        "            correct += (preds == batch_y).sum()\n",
        "            total += len(batch_y)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "accuracy = fast_test(agent, X_test, y_test)\n",
        "print(\"Fast Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB5PWFqaokc0",
        "outputId": "9cabe8a8-3865-45cb-89a3-5ac0a636bf54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast Test Accuracy: 0.8306970890478996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Enhanced Dueling DQN with Dropout + BatchNorm\n",
        "\n",
        "class EnhancedDQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(EnhancedDQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # Dueling streams\n",
        "        self.value_stream = nn.Linear(128, 1)\n",
        "        self.advantage_stream = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "        q_vals = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
        "        return q_vals"
      ],
      "metadata": {
        "id": "2Rx0oRHg86Ue"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Upgrade Agent to use EnhancedDQN\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, batch_size=128, buffer_size=100000):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.policy_net = EnhancedDQN(state_dim, action_dim).to(self.device)\n",
        "        self.target_net = EnhancedDQN(state_dim, action_dim).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "\n",
        "        self.update_target()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def act(self, state, epsilon=0.1):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        q_vals = self.policy_net(state)\n",
        "        return torch.argmax(q_vals, dim=1).item()\n",
        "\n",
        "    def remember(self, s, a, r, s_next, done):\n",
        "        self.memory.append((s, a, r, s_next, done))\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Q(s, a)\n",
        "        q_vals = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "        # Double DQN\n",
        "        next_actions = torch.argmax(self.policy_net(next_states), dim=1)\n",
        "        next_q_vals = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
        "        target = rewards + (1 - dones) * self.gamma * next_q_vals\n",
        "\n",
        "        loss = F.mse_loss(q_vals, target.detach())\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "0GrwRhMpAYW_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Enhanced Evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "def evaluate(agent, X_test, y_test, batch_size=1024):\n",
        "    device = agent.device\n",
        "    preds_all = []\n",
        "    true_all = []\n",
        "\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    agent.policy_net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_test), batch_size):\n",
        "            batch_X = X_test_tensor[i:i+batch_size]\n",
        "            outputs = agent.policy_net(batch_X)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            preds_all.extend(preds)\n",
        "            true_all.extend(y_test[i:i+batch_size])\n",
        "\n",
        "    # Convert to arrays\n",
        "    preds_all = np.array(preds_all)\n",
        "    true_all = np.array(true_all)\n",
        "\n",
        "    acc = accuracy_score(true_all, preds_all)\n",
        "    prec = precision_score(true_all, preds_all, average='weighted', zero_division=0)\n",
        "    rec = recall_score(true_all, preds_all, average='weighted')\n",
        "    f1 = f1_score(true_all, preds_all, average='weighted')\n",
        "\n",
        "    print(\"\\n Classification Report:\\n\", classification_report(true_all, preds_all, target_names=le.classes_))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(true_all, preds_all))\n",
        "    print(f\" Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "    return acc, prec, rec, f1\n",
        "\n",
        "# Run evaluation\n",
        "acc, prec, rec, f1 = evaluate(agent, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv0vJpnNAcul",
        "outputId": "c62038fc-430d-417f-c4f2-c67821af4516"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "                  Benign       0.83      1.00      0.91   2696942\n",
            "                     Bot       0.00      0.00      0.00     57238\n",
            "        Brute Force -Web       0.00      0.00      0.00       122\n",
            "        Brute Force -XSS       0.00      0.00      0.00        46\n",
            "        DDOS attack-HOIC       0.00      0.00      0.00    137203\n",
            "    DDOS attack-LOIC-UDP       0.00      0.00      0.00       346\n",
            "  DDoS attacks-LOIC-HTTP       0.00      0.00      0.00    115238\n",
            "   DoS attacks-GoldenEye       0.00      0.00      0.00      8302\n",
            "        DoS attacks-Hulk       0.00      0.00      0.00     92382\n",
            "DoS attacks-SlowHTTPTest       0.00      0.00      0.00     27978\n",
            "   DoS attacks-Slowloris       0.00      0.00      0.00      2198\n",
            "          FTP-BruteForce       0.00      0.00      0.00     38672\n",
            "           Infilteration       0.00      0.00      0.00     32387\n",
            "                   Label       0.00      0.00      0.00        12\n",
            "           SQL Injection       0.00      0.00      0.00        17\n",
            "          SSH-Bruteforce       0.00      0.00      0.00     37518\n",
            "\n",
            "                accuracy                           0.83   3246601\n",
            "               macro avg       0.05      0.06      0.06   3246601\n",
            "            weighted avg       0.69      0.83      0.75   3246601\n",
            "\n",
            "Confusion Matrix:\n",
            " [[2696942       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [  57238       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [    122       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [     46       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [ 137203       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [    346       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [ 115238       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [   8302       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [  92382       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [  27978       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [   2198       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [  38672       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [  32387       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [     12       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [     17       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]\n",
            " [  37518       0       0       0       0       0       0       0       0\n",
            "        0       0       0       0       0       0       0]]\n",
            " Accuracy: 0.8307, Precision: 0.6901, Recall: 0.8307, F1: 0.7539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q imbalanced-learn --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "079AzzWzAkAm",
        "outputId": "7687f1c8-9d68-4c52-9bf3-f6238b512062"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/240.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/240.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports (add to top of file if not present)\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import os\n",
        "\n",
        "# 1) Optional: Binary mapping (if you prefer Binary detection)\n",
        "\n",
        "# Uncomment if you want to convert to binary (Benign vs Attack)\n",
        "# y_train_bin = np.where(y_train == le.transform(['Benign'])[0], 0, 1)\n",
        "# y_test_bin  = np.where(y_test  == le.transform(['Benign'])[0], 0, 1)\n",
        "# Then set env.action_space = 2 and proceed. For now we keep multiclass.\n",
        "\n",
        "# 2) Resample training set with RandomOverSampler (balances minority classes)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "print(\"After oversampling, counts:\", np.bincount(y_train_res))\n",
        "\n",
        "# replace training data used for supervised pretraining\n",
        "X_pre = X_train_res\n",
        "y_pre = y_train_res\n",
        "\n",
        "# 3) Compute class weights (for loss)\n",
        "\n",
        "classes = np.unique(y_pre)\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_pre)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "print(\"Class weights:\", dict(zip(le.classes_, class_weights)))\n",
        "\n",
        "# 4) Focal Loss (optional but helpful on imbalanced multiclass)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logp = -self.ce(input, target)\n",
        "        p = torch.exp(logp)\n",
        "        loss = -((1 - p) ** self.gamma) * logp\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        return loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loznK9tFDCUt",
        "outputId": "87873e19-e795-4d77-a465-c87c7136f232"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After oversampling, counts: [10787766 10787766 10787766 10787766 10787766 10787766 10787766 10787766\n",
            " 10787766 10787766 10787766 10787766 10787766 10787766 10787766 10787766]\n",
            "Class weights: {'Benign': np.float64(1.0), 'Bot': np.float64(1.0), 'Brute Force -Web': np.float64(1.0), 'Brute Force -XSS': np.float64(1.0), 'DDOS attack-HOIC': np.float64(1.0), 'DDOS attack-LOIC-UDP': np.float64(1.0), 'DDoS attacks-LOIC-HTTP': np.float64(1.0), 'DoS attacks-GoldenEye': np.float64(1.0), 'DoS attacks-Hulk': np.float64(1.0), 'DoS attacks-SlowHTTPTest': np.float64(1.0), 'DoS attacks-Slowloris': np.float64(1.0), 'FTP-BruteForce': np.float64(1.0), 'Infilteration': np.float64(1.0), 'Label': np.float64(1.0), 'SQL Injection': np.float64(1.0), 'SSH-Bruteforce': np.float64(1.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversample (balance classes)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "print(\"Resampled counts:\", np.bincount(y_train_res))\n",
        "\n",
        "# Class weights\n",
        "classes = np.unique(y_train_res)\n",
        "class_weights = compute_class_weight(\"balanced\", classes=classes, y=y_train_res)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, weight=None):\n",
        "        super().__init__()\n",
        "        self.ce = nn.CrossEntropyLoss(weight=weight, reduction='none')\n",
        "        self.gamma = gamma\n",
        "    def forward(self, input, target):\n",
        "        logp = -self.ce(input, target)\n",
        "        p = torch.exp(logp)\n",
        "        return (-(1 - p) ** self.gamma * logp).mean()\n",
        "\n",
        "# Supervised Pretraining\n",
        "def supervised_pretrain(model, X, y, epochs=3, bs=2048, lr=1e-4):\n",
        "    model.to(device).train()\n",
        "    ds = TensorDataset(torch.FloatTensor(X), torch.LongTensor(y))\n",
        "    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    crit = FocalLoss(weight=class_weights_tensor)\n",
        "    for ep in range(epochs):\n",
        "        loss_sum = 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb)\n",
        "            loss = crit(out, yb)\n",
        "            loss.backward(); opt.step()\n",
        "            loss_sum += loss.item() * len(xb)\n",
        "        print(f\"Pretrain {ep+1}/{epochs} - Loss: {loss_sum/len(ds):.4f}\")\n",
        "\n",
        "supervised_pretrain(agent.policy_net, X_train_res, y_train_res, epochs=3, bs=4096)\n",
        "\n",
        "agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
        "\n",
        "# Simple Balanced Replay\n",
        "class BalancedReplay:\n",
        "    def __init__(self, maxlen=50000): self.mem = defaultdict(lambda: deque(maxlen=maxlen))\n",
        "    def add(self, sample, label): self.mem[int(label)].append(sample)\n",
        "    def sample(self, bs):\n",
        "        classes = list(self.mem.keys()); per_cls = max(1, bs//len(classes))\n",
        "        batch = []\n",
        "        for c in classes:\n",
        "            items = list(self.mem[c])\n",
        "            if items: batch.extend(random.choices(items, k=per_cls))\n",
        "        return random.sample(batch, min(len(batch), bs))\n",
        "\n",
        "agent.replay_memory = BalancedReplay()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ_tqUoNJxaV",
        "outputId": "2302555c-44a2-4c81-f94e-611f318f7caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resampled counts: [10787766 10787766 10787766 10787766 10787766 10787766 10787766 10787766\n",
            " 10787766 10787766 10787766 10787766 10787766 10787766 10787766 10787766]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "episodes, epsilon, eps_min, eps_decay = 5, 1.0, 0.1, 0.9\n",
        "for ep in range(episodes):\n",
        "    s, done, tot_r = env.reset(), False, 0\n",
        "    while not done:\n",
        "        a = agent.act(s, epsilon)\n",
        "        s2, r, done, _ = env.step(a)\n",
        "        label = y_train[env.current_index-1]\n",
        "        agent.replay_memory.add((s,a,r,s2,done), label)\n",
        "        if len(agent.replay_memory.mem)>0:\n",
        "            mb = agent.replay_memory.sample(agent.batch_size)\n",
        "            if mb: agent.replay_balanced()\n",
        "        s, tot_r = s2, tot_r+r\n",
        "    agent.update_target()\n",
        "    epsilon = max(eps_min, epsilon*eps_decay)\n",
        "    print(f\"Episode {ep+1}/{episodes} | Reward={tot_r} | Eps={epsilon:.2f}\")\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(agent, X, y, bs=4096):\n",
        "    agent.policy_net.eval(); preds=[]\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X), bs):\n",
        "            xb = torch.FloatTensor(X[i:i+bs]).to(device)\n",
        "            preds.extend(torch.argmax(agent.policy_net(xb),1).cpu().numpy())\n",
        "    print(classification_report(y, preds, target_names=le.classes_, zero_division=0))\n",
        "    print(\"CM:\", confusion_matrix(y, preds).shape)\n",
        "    return np.array(preds)\n",
        "\n",
        "preds = evaluate(agent, X_test, y_test)"
      ],
      "metadata": {
        "id": "mVLqp0EaJ94N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fresh Start"
      ],
      "metadata": {
        "id": "-qo8T1crsis6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "TExb8sg3slvI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load Dataset\n",
        "path = \"/content/drive/MyDrive/RL/\"\n",
        "files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "\n",
        "df_list = []\n",
        "for f in files:\n",
        "    try:\n",
        "        df_list.append(pd.read_csv(f))\n",
        "    except:\n",
        "        print(\"Error reading:\", f)\n",
        "\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYRCO2FWsnS4",
        "outputId": "b6876799-8cfa-4b9d-9f5a-8d46a8bbe961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4046094165.py:8: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_list.append(pd.read_csv(f))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocessing\n",
        "df = df.dropna(axis=1, how=\"all\").dropna()\n",
        "\n",
        "# Encode labels (Attack / Normal)\n",
        "if \"Label\" in df.columns:\n",
        "    df['Label'] = df['Label'].str.strip()\n",
        "    encoder = LabelEncoder()\n",
        "    df['Label'] = encoder.fit_transform(df['Label'])\n",
        "else:\n",
        "    raise ValueError(\"Label column not found in dataset!\")\n",
        "\n",
        "y = df['Label']\n",
        "X = df.drop(columns=['Label'])\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "_JWGdEe0sr-y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}